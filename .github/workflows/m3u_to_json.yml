# .github/workflows/m3u_to_json.yml
name: Convertir M3U (URL) a JSON + separar series (con verificacion online)

permissions:
  contents: write

on:
  workflow_dispatch:
    inputs:
      m3u_url:
        description: "URL del archivo .m3u/.m3u8 (texto #EXTM3U)"
        required: true
      output_dir:
        description: "Carpeta de salida en la raiz"
        required: false
        default: "m3u_convert"
      chunk_size:
        description: "Max. cantidad de items (peliculas) por cada JSON part"
        required: false
        default: "2000"
      verify_urls:
        description: "Verificar si la URL del video esta online (true/false). Si esta offline, se ignora."
        required: false
        default: "false"
      verify_timeout:
        description: "Timeout por URL (segundos)"
        required: false
        default: "6"
      verify_workers:
        description: "Cantidad de hilos para verificacion (concurrencia)"
        required: false
        default: "30"

jobs:
  build:
    runs-on: ubuntu-latest

    steps:
      - name: Checkout
        uses: actions/checkout@v4
        with:
          persist-credentials: true

      - name: Setup Python
        uses: actions/setup-python@v5
        with:
          python-version: "3.x"

      - name: Install dependencies
        run: |
          pip install --no-cache-dir requests

      - name: Convert M3U -> JSON + split series/movies (+ optional verify)
        env:
          M3U_URL: ${{ github.event.inputs.m3u_url }}
          OUTPUT_DIR: ${{ github.event.inputs.output_dir }}
          CHUNK_SIZE: ${{ github.event.inputs.chunk_size }}
          VERIFY_URLS: ${{ github.event.inputs.verify_urls }}
          VERIFY_TIMEOUT: ${{ github.event.inputs.verify_timeout }}
          VERIFY_WORKERS: ${{ github.event.inputs.verify_workers }}
        run: |
          python <<'PY'
          import os
          import re
          import json
          import time
          import requests
          from collections import OrderedDict
          from concurrent.futures import ThreadPoolExecutor, as_completed

          M3U_URL = os.environ.get("M3U_URL", "").strip()
          OUTPUT_DIR = os.environ.get("OUTPUT_DIR", "m3u_convert").strip()
          CHUNK_SIZE = int((os.environ.get("CHUNK_SIZE") or "2000").strip())

          VERIFY_URLS = (os.environ.get("VERIFY_URLS", "false").strip().lower() == "true")
          VERIFY_TIMEOUT = float(os.environ.get("VERIFY_TIMEOUT") or "6")
          VERIFY_WORKERS = int(os.environ.get("VERIFY_WORKERS") or "30")

          if not M3U_URL:
              raise SystemExit("Falta M3U_URL")

          os.makedirs(OUTPUT_DIR, exist_ok=True)

          # ---------- Util: parse headers embebidos en URL (formato VLC/OTT) ----------
          # Ej: http://host/stream.m3u8|User-Agent=xxx&Referer=https://site
          def split_url_pipe_headers(raw_url: str):
              url = raw_url.strip()
              headers = {}
              if "|" not in url:
                  return url, headers
              base, meta = url.split("|", 1)
              # meta: Key=Val&Key2=Val2 ...
              for part in meta.split("&"):
                  if "=" in part:
                      k, v = part.split("=", 1)
                      k = k.strip()
                      v = v.strip()
                      if k and v:
                          headers[k] = v
              return base.strip(), headers

          # ---------- Ano (si aparece en el titulo) ----------
          YEAR_RE = re.compile(r"\b(19\d{2}|20\d{2})\b")

          def extract_year(title: str) -> str:
              if not title:
                  return ""
              yrs = YEAR_RE.findall(title)
              return yrs[-1] if yrs else ""

          # ---------- Heuristicas SERIES ----------
          SERIES_GROUP_HINTS = [
              "temporada", "season", "serie", "series", "tv", "episodios", "capitulos", "capítulos",
              "capitulo", "capítulo"
          ]

          # patrones claros de episodio/capitulo
          SERIES_PATTERNS = [
              re.compile(r"\b\d{1,2}x\d{1,3}\b", re.IGNORECASE),                 # 1x01, 2x10
              re.compile(r"\bs\d{1,2}\s*e\d{1,3}\b", re.IGNORECASE),             # S01E02
              re.compile(r"\bcap(?:itulo|ítulo)?\.?\s*\d+\b", re.IGNORECASE),    # capitulo 10 / cap. 10
              re.compile(r"\bepisodio\.?\s*\d+\b", re.IGNORECASE),               # episodio 1
              re.compile(r"\bepisode\.?\s*\d+\b", re.IGNORECASE),                # episode 1
              re.compile(r"\bep\.?\s*0*\d+\b", re.IGNORECASE),                   # ep01, EP 2
              re.compile(r"\be\.?\s*0*\d+\b", re.IGNORECASE),                    # e01, E2
              re.compile(r"\bova\s*\d*\b", re.IGNORECASE),                       # ova / ova 1
              re.compile(r"\b(oav|special|especial|extra)\b", re.IGNORECASE),    # especiales
              re.compile(r"\btemporada\s*\d+\b", re.IGNORECASE),                 # temporada 1
              re.compile(r"\bseason\s*\d+\b", re.IGNORECASE),                    # season 2
              re.compile(r"^\s*\d{1,3}\b", re.IGNORECASE),
          ]

          # TEMPORADA / SEASON / T01 / S01 sueltos
          SEASON_IN_TITLE = [
              re.compile(r"\btemporada\b", re.IGNORECASE),
              re.compile(r"\bseason\b", re.IGNORECASE),
              re.compile(r"\btemp\.?\s*\d+\b", re.IGNORECASE),                   # temp 1
              re.compile(r"\bt\s*0?\d{1,2}\b", re.IGNORECASE),                    # T1 / T01
              re.compile(r"\bs\s*0?\d{1,2}\b", re.IGNORECASE),                    # S1 / S01 (solo)
          ]

          # "10 - Nombre" / "01 - ..." / "02. ..." / "03) ..." / "04 _ ..."
          LEADING_EP_NUMBER = re.compile(r"^\s*\d{1,3}\s*[-–._)]\s+\S+", re.IGNORECASE)

          # "01 02" / "01-02" / "01_02" (dos numeros cortos seguidos)
          TWO_SHORT_NUMBERS = re.compile(r"\b0?\d{1,2}\s*[-_ ]\s*0?\d{1,2}\b")

          # Solo numeros tipo "01" "02" o "001" al inicio (muy comun en listas de episodios)
          LEADING_ONLY_NUM = re.compile(r"^\s*0?\d{1,3}\b")

          # Tokens sueltos S01/T01 entre separadores raros
          LOOSE_SEASON_TOKEN = re.compile(r"(^|[\s._-])([st])\s*0?\d{1,2}($|[\s._-])", re.IGNORECASE)

          def looks_like_series(title: str, group_title: str) -> bool:
              t = (title or "").strip()
              gt = (group_title or "").strip().lower()
              tl = t.lower()

              # 1) group-title fuerte
              for hint in SERIES_GROUP_HINTS:
                  if hint in gt:
                      return True

              # 2) temporada/season/Txx/Sxx en titulo
              for p in SEASON_IN_TITLE:
                  if p.search(t):
                      return True

              if LOOSE_SEASON_TOKEN.search(t):
                  return True

              # 3) patrones claros de episodio
              for p in SERIES_PATTERNS:
                  if p.search(t):
                      return True

              # 4) "10 - ..." al inicio
              if LEADING_EP_NUMBER.search(t):
                  return True

              # 5) "01 02" / "01-02" / "01_02" con contexto
              if TWO_SHORT_NUMBERS.search(t):
                  if gt or len(t) <= 45 or any(k in tl for k in ["temp", "season", "cap", "epis", "episode", "ep ", "e0", "x"]):
                      return True

              # 6) empieza con numero corto y el titulo es corto (tipico "01 - Titulo")
              if LEADING_ONLY_NUM.search(t) and len(t) <= 55:
                  # Para no marcar peliculas tipo "1917", exigimos que NO sea anio aislado
                  if not re.fullmatch(r"(19\d{2}|20\d{2})", t.strip()):
                      return True

              # 7) palabras clave
              if any(k in tl for k in ["temporada", "season", "capitulo", "capítulo", "episodio", "episode"]):
                  return True

              return False

          # ---------- Parser EXTINF ----------
          # Ej: #EXTINF:-1 tvg-logo="..." group-title="...",Titulo
          ATTR_RE = re.compile(r'([\w-]+)\s*=\s*"([^"]*)"')

          def parse_extinf(line: str):
              if "," in line:
                  left, title = line.split(",", 1)
              else:
                  left, title = line, ""
              attrs = dict(ATTR_RE.findall(left))
              return attrs, (title or "").strip()

          # ---------- Parse EXT Vlc Opt (headers) ----------
          # Ej: #EXTVLCOPT:http-user-agent=...
          #     #EXTVLCOPT:http-referrer=...
          def parse_extvlcopt(line: str):
              # retorna (k,v) o (None,None)
              prefix = "#EXTVLCOPT:"
              if not line.startswith(prefix):
                  return None, None
              rest = line[len(prefix):].strip()
              if "=" not in rest:
                  return None, None
              k, v = rest.split("=", 1)
              return k.strip().lower(), v.strip()

          def build_verify_headers(entry):
              # defaults
              headers = {
                  "User-Agent": entry.get("user_agent") or "Mozilla/5.0",
                  "Accept": "*/*",
                  "Connection": "close",
              }
              ref = entry.get("referer") or ""
              if ref:
                  headers["Referer"] = ref
              return headers

          # ---------- Verificacion online (HEAD + fallback GET Range) ----------
          def _guess_kind_from_url(url: str) -> str:
    u = (url or "").lower()
    if ".m3u8" in u:
        return "m3u8"
    if ".mpd" in u:
        return "mpd"
    if any(ext in u for ext in [".mp4", ".m4v", ".mov"]):
        return "mp4"
    if ".mkv" in u:
        return "mkv"
    if ".webm" in u:
        return "webm"
    if ".ts" in u:
        return "ts"
    return "unknown"

def _looks_like_media_bytes(kind: str, b: bytes) -> bool:
    if not b:
        return False
    head = b[:256].lstrip()

    # HLS
    if kind == "m3u8":
        return head.startswith(b"#EXTM3U")
    # DASH
    if kind == "mpd":
        return head.startswith(b"<?xml") or b"<MPD" in head[:200] or head.startswith(b"<MPD")
    # TS
    if kind == "ts":
        return b[:1] == b"\x47"
    # MKV (EBML)
    if kind == "mkv":
        return b.startswith(b"\x1A\x45\xDF\xA3")
    # MP4 / ISO BMFF: 'ftyp' suele estar en los primeros 4-16 bytes
    if kind == "mp4":
        return (b.find(b"ftyp", 0, 32) != -1)
    # WEBM también es EBML (como MKV)
    if kind == "webm":
        return b.startswith(b"\x1A\x45\xDF\xA3")

    # Unknown: si no es texto/html al menos
    return True

def _is_probably_html(content_type: str, b: bytes) -> bool:
    ct = (content_type or "").lower()
    if "text/html" in ct:
        return True
    # algunos devuelven text/plain con HTML
    if ("text/" in ct or "application/xhtml" in ct) and b:
        s = b[:200].lstrip().lower()
        return s.startswith(b"<!doctype html") or s.startswith(b"<html") or b"<html" in s
    return False

def is_url_online(url: str, headers: dict) -> bool:
    """
    Online = accesible Y parece ser media real (no HTML de error).
    Estrategia:
      1) HEAD (rápido) para redirecciones / status obvio
      2) GET Range 0-1023 para validar contenido (Content-Type + magic bytes)
    """
    if not url:
        return False

    kind = _guess_kind_from_url(url)

    # defaults seguros
    headers = dict(headers or {})
    headers.setdefault("User-Agent", "Mozilla/5.0")
    headers.setdefault("Accept", "*/*")
    headers.setdefault("Connection", "close")

    # 1) HEAD
    try:
        r = requests.head(url, allow_redirects=True, timeout=VERIFY_TIMEOUT, headers=headers)
        code = r.status_code

        # si es 404/410 -> offline
        if code in (404, 410):
            return False

        # si 2xx/3xx seguimos, pero validamos con GET
        # si 401/403/405: NO lo damos por online (porque para vos "online" = usable)
        if code in (401, 403, 405):
            return False

        # si 5xx o otros raros -> fallback GET igual (a veces HEAD falla)
    except requests.RequestException:
        # HEAD falló, probamos GET
        pass

    # 2) GET mínimo con Range
    try:
        headers2 = dict(headers)
        headers2["Range"] = "bytes=0-1023"
        r2 = requests.get(url, allow_redirects=True, timeout=VERIFY_TIMEOUT, headers=headers2, stream=True)
        code2 = r2.status_code

        # aceptamos 200/206 (206 típico por Range)
        if code2 not in (200, 206):
            return False

        ct = r2.headers.get("Content-Type", "")
        # leer un poco del body (sin bajar todo)
        chunk = b""
        for part in r2.iter_content(chunk_size=1024):
            chunk = part or b""
            break

        # Si parece HTML (error page), descartamos
        if _is_probably_html(ct, chunk):
            return False

        # Si es un tipo conocido, validamos firma/magic bytes
        if kind != "unknown":
            return _looks_like_media_bytes(kind, chunk)

        # Unknown: si no es HTML, lo damos por válido
        return True

    except requests.RequestException:
        return False


          # ---------- Descargar M3U ----------
          resp = requests.get(M3U_URL, timeout=120)
          resp.raise_for_status()
          text = resp.text

          lines = [ln.strip() for ln in text.splitlines() if ln.strip()]

          all_entries = []

          current_extinf = None
          current_attrs = None
          current_title = None
          current_opts_lines = []   # para preservar en M3U
          current_user_agent = ""
          current_referer = ""

          for ln in lines:
              if ln.startswith("#EXTM3U"):
                  continue

              if ln.startswith("#EXTINF"):
                  current_extinf = ln
                  current_attrs, current_title = parse_extinf(ln)
                  current_opts_lines = []
                  current_user_agent = ""
                  current_referer = ""
                  continue

              # headers opcionales entre EXTINF y URL
              if current_extinf and ln.startswith("#EXTVLCOPT:"):
                  k, v = parse_extvlcopt(ln)
                  if k and v:
                      current_opts_lines.append(ln)
                      if k == "http-user-agent":
                          current_user_agent = v
                      elif k == "http-referrer":
                          current_referer = v
                  continue

              # URL (linea siguiente al EXTINF)
              if current_extinf and not ln.startswith("#"):
                  raw_url = ln.strip()

                  # Soportar URL|headers
                  video_url, pipe_headers = split_url_pipe_headers(raw_url)

                  # headers desde pipe si existen
                  if not current_user_agent and ("User-Agent" in pipe_headers or "user-agent" in pipe_headers):
                      current_user_agent = pipe_headers.get("User-Agent") or pipe_headers.get("user-agent") or ""
                  if not current_referer and ("Referer" in pipe_headers or "referer" in pipe_headers):
                      current_referer = pipe_headers.get("Referer") or pipe_headers.get("referer") or ""

                  group_title = (current_attrs.get("group-title") or "").strip()
                  tvg_logo = (current_attrs.get("tvg-logo") or "").strip()

                  title = (current_title or "").strip()
                  if not title:
                      title = (current_attrs.get("tvg-name") or current_attrs.get("tvg-id") or "").strip()

                  entry = {
                      "extinf": current_extinf,
                      "opts_lines": list(current_opts_lines),
                      "title": title,
                      "url": video_url,
                      "group_title": group_title,
                      "tvg_logo": tvg_logo,
                      "anio": extract_year(title),
                      "user_agent": current_user_agent,
                      "referer": current_referer,
                  }
                  all_entries.append(entry)

                  current_extinf = None
                  current_attrs = None
                  current_title = None
                  current_opts_lines = []
                  current_user_agent = ""
                  current_referer = ""

          # ---------- Verificar URLs (opcional) ----------
          if VERIFY_URLS and all_entries:
              # clave por (url, ua, ref) para minimizar duplicados
              unique_keys = OrderedDict()
              for e in all_entries:
                  key = (e["url"], e.get("user_agent") or "", e.get("referer") or "")
                  unique_keys.setdefault(key, e)

              results = {}
              keys = list(unique_keys.keys())

              print(f"Verificando URLs: {len(keys)} unicas (workers={VERIFY_WORKERS}, timeout={VERIFY_TIMEOUT}s)")

              with ThreadPoolExecutor(max_workers=VERIFY_WORKERS) as ex:
                  future_map = {}
                  for key in keys:
                      url, ua, ref = key
                      tmp_entry = {"url": url, "user_agent": ua, "referer": ref}
                      headers = build_verify_headers(tmp_entry)
                      future_map[ex.submit(is_url_online, url, headers)] = key

                  done = 0
                  for fut in as_completed(future_map):
                      key = future_map[fut]
                      ok = False
                      try:
                          ok = bool(fut.result())
                      except Exception:
                          ok = False
                      results[key] = ok
                      done += 1
                      if done % 500 == 0:
                          print(f"  progreso: {done}/{len(keys)}")

              filtered = []
              for e in all_entries:
                  key = (e["url"], e.get("user_agent") or "", e.get("referer") or "")
                  if results.get(key, False):
                      filtered.append(e)
              dropped = len(all_entries) - len(filtered)
              print(f"URLs offline ignoradas: {dropped}")
              all_entries = filtered

          # ---------- Separar series/peliculas ----------
          movies_entries = []
          series_entries = []
          for e in all_entries:
              if looks_like_series(e["title"], e["group_title"]):
                  series_entries.append(e)
              else:
                  movies_entries.append(e)

          # ---------- Escribir M3U separados (preservando EXTVLCOPT si existia) ----------
          def write_m3u(path, entries):
              out = ["#EXTM3U"]
              for e in entries:
                  out.append(e["extinf"])
                  for opt in e.get("opts_lines", []):
                      out.append(opt)
                  out.append(e["url"])
              with open(path, "w", encoding="utf-8") as f:
                  f.write("\n".join(out) + "\n")

          write_m3u(os.path.join(OUTPUT_DIR, "peliculas.m3u"), movies_entries)
          write_m3u(os.path.join(OUTPUT_DIR, "series.m3u"), series_entries)

          # ---------- JSON con TODOS los campos (como pediste) ----------
          # record = { "name": group-title, "samples": [{...}] }
          def sample_from_entry(e, tipo: str):
              return {
                  "name": e["title"],
                  "url": e["url"],
                  "icono": e["tvg_logo"] or "",          # tvg-logo -> icono
                  "iconoHorizontal": "",
                  "iconpng": "",
                  "type": tipo,                          # PELICULA o SERIE
                  "descripcion": "",
                  "anio": e.get("anio") or "",
                  "genero": "",
                  "duracion": ""
              }

          def to_grouped_records(entries, tipo: str):
              groups = OrderedDict()
              for e in entries:
                  g = e["group_title"]  # group-title -> record.name
                  if g not in groups:
                      groups[g] = []
                  groups[g].append(sample_from_entry(e, tipo))
              return [{"name": g, "samples": samples} for g, samples in groups.items()]

          movies_records = to_grouped_records(movies_entries, "PELICULA")
          series_records = to_grouped_records(series_entries, "SERIE")

          # ---------- Split JSON de peliculas en partes ----------
          def split_records_by_samples(records, max_samples):
              parts = []
              cur = []
              cur_count = 0
              for r in records:
                  s = r.get("samples", [])
                  if cur and (cur_count + len(s) > max_samples):
                      parts.append(cur)
                      cur = []
                      cur_count = 0
                  cur.append(r)
                  cur_count += len(s)
              if cur:
                  parts.append(cur)
              return parts

          parts = split_records_by_samples(movies_records, CHUNK_SIZE)

          if len(parts) == 1:
              with open(os.path.join(OUTPUT_DIR, "peliculas.json"), "w", encoding="utf-8") as f:
                  json.dump(parts[0], f, indent=2, ensure_ascii=False)
          else:
              for i, part in enumerate(parts, start=1):
                  with open(os.path.join(OUTPUT_DIR, f"peliculas_part{i:03d}.json"), "w", encoding="utf-8") as f:
                      json.dump(part, f, indent=2, ensure_ascii=False)
              manifest = {
                  "total_parts": len(parts),
                  "chunk_size_samples": CHUNK_SIZE,
                  "files": [f"peliculas_part{i:03d}.json" for i in range(1, len(parts) + 1)]
              }
              with open(os.path.join(OUTPUT_DIR, "peliculas_manifest.json"), "w", encoding="utf-8") as f:
                  json.dump(manifest, f, indent=2, ensure_ascii=False)

          # Series JSON completo
          with open(os.path.join(OUTPUT_DIR, "series.json"), "w", encoding="utf-8") as f:
              json.dump(series_records, f, indent=2, ensure_ascii=False)

          print(f"M3U URL: {M3U_URL}")
          print(f"VERIFY_URLS: {VERIFY_URLS}")
          print(f"Peliculas: {len(movies_entries)} entries")
          print(f"Series: {len(series_entries)} entries")
          print(f"Peliculas JSON parts: {len(parts)} (chunk_size={CHUNK_SIZE})")
          print(f"Salida en: {OUTPUT_DIR}")
          PY

      - name: Commit and Push
        run: |
          git config user.name "github-actions[bot]"
          git config user.email "github-actions[bot]@users.noreply.github.com"
          git add "${{ github.event.inputs.output_dir }}"
          if ! git diff --cached --exit-code; then
            git commit -m "Convertir M3U a JSON y separar series/peliculas (+verify opcional)"
            git push
          else
            echo "No hay cambios"
          fi
