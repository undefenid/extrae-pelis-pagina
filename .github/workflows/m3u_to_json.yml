# .github/workflows/m3u_to_json.yml
name: Convertir M3U (URL) a JSON + separar series (afinada + validar URLs)

permissions:
  contents: write

on:
  workflow_dispatch:
    inputs:
      m3u_url:
        description: "URL del archivo .m3u/.m3u8 (texto #EXTM3U)"
        required: true
      output_dir:
        description: "Carpeta de salida en la raíz"
        required: false
        default: "m3u_convert"
      chunk_size:
        description: "Máx. cantidad de items (películas) por cada JSON part"
        required: false
        default: "2000"
      check_urls:
        description: "Verificar si la URL del video responde (si no, se ignora el item)"
        required: false
        default: "true"
      max_workers:
        description: "Cantidad de hilos para validar URLs (más rápido, pero más carga)"
        required: false
        default: "20"
      timeout_sec:
        description: "Timeout por URL (segundos)"
        required: false
        default: "8"

jobs:
  build:
    runs-on: ubuntu-latest

    steps:
      - name: Checkout
        uses: actions/checkout@v4
        with:
          persist-credentials: true

      - name: Setup Python
        uses: actions/setup-python@v5
        with:
          python-version: "3.x"

      - name: Install dependencies
        run: |
          pip install --no-cache-dir requests

      - name: Convert M3U -> JSON + split series/movies + url check
        env:
          M3U_URL: ${{ github.event.inputs.m3u_url }}
          OUTPUT_DIR: ${{ github.event.inputs.output_dir }}
          CHUNK_SIZE: ${{ github.event.inputs.chunk_size }}
          CHECK_URLS: ${{ github.event.inputs.check_urls }}
          MAX_WORKERS: ${{ github.event.inputs.max_workers }}
          TIMEOUT_SEC: ${{ github.event.inputs.timeout_sec }}
        run: |
          python <<'PY'
          import os
          import re
          import json
          import requests
          import unicodedata
          from collections import OrderedDict
          from concurrent.futures import ThreadPoolExecutor, as_completed

          M3U_URL = os.environ.get("M3U_URL", "").strip()
          OUTPUT_DIR = os.environ.get("OUTPUT_DIR", "m3u_convert").strip()
          CHUNK_SIZE = int((os.environ.get("CHUNK_SIZE") or "2000").strip())

          CHECK_URLS = (os.environ.get("CHECK_URLS") or "true").strip().lower() in ("1", "true", "yes", "y", "si", "sí")
          MAX_WORKERS = int((os.environ.get("MAX_WORKERS") or "20").strip())
          TIMEOUT_SEC = float((os.environ.get("TIMEOUT_SEC") or "8").strip())

          if not M3U_URL:
              raise SystemExit("Falta M3U_URL")

          os.makedirs(OUTPUT_DIR, exist_ok=True)

          # ------------------ Helpers de texto ------------------
          def norm(s: str) -> str:
              s = s or ""
              s = "".join(c for c in unicodedata.normalize("NFD", s) if unicodedata.category(c) != "Mn")
              return s.lower().strip()

          # ------------------ Año (si aparece en el título) ------------------
          YEAR_RE = re.compile(r"\b(19\d{2}|20\d{2})\b")

          def extract_year(title: str) -> str:
              if not title:
                  return ""
              yrs = YEAR_RE.findall(title)
              return yrs[-1] if yrs else ""

          # ------------------ Parser EXTINF ------------------
          # Ej: #EXTINF:-1 tvg-logo="..." group-title="...",Titulo
          ATTR_RE = re.compile(r'([\w-]+)\s*=\s*"([^"]*)"')

          def parse_extinf(line: str):
              if "," in line:
                  left, title = line.split(",", 1)
              else:
                  left, title = line, ""
              attrs = dict(ATTR_RE.findall(left))
              return attrs, (title or "").strip()

          # ------------------ Clasificador SERIES vs PELÍCULAS (score) ------------------
          # Hints en group-title
          SERIES_GROUP_HINTS = [
              "temporada", "season", "serie", "series", "tv", "tvshow", "episodios", "capitulos", "capitulo", "caps"
          ]
          MOVIE_GROUP_HINTS = [
              "pelicula", "peliculas", "movie", "movies", "cine", "film", "films"
          ]

          # Señales fuertes de episodio
          P_SXXEYY = re.compile(r"\bs\d{1,2}\s*e\d{1,3}\b", re.IGNORECASE)            # S01E02
          P_XXxYY  = re.compile(r"\b\d{1,2}x\d{1,3}\b", re.IGNORECASE)               # 1x01
          P_EP     = re.compile(r"\b(ep|episodio|episode)\s*\.?\s*0*\d+\b", re.IGNORECASE)  # ep01 / episodio 2
          P_E      = re.compile(r"\b(e)\s*\.?\s*0*\d+\b", re.IGNORECASE)             # e01 (suave)
          P_CAP    = re.compile(r"\b(cap|capitulo|capítulo|chapter)\s*\.?\s*\d+\b", re.IGNORECASE)

          # Temporada en título
          P_TEMPORADA = re.compile(r"\b(temporada|season|temp)\b", re.IGNORECASE)
          P_TNN = re.compile(r"(^|[\s._-])t\s*0?\d{1,2}($|[\s._-])", re.IGNORECASE)  # T1 / T01
          P_SNN = re.compile(r"(^|[\s._-])s\s*0?\d{1,2}($|[\s._-])", re.IGNORECASE)  # S1 / S01 (solo)

          # Formatos raros: "01 02" / "01-02" / "01_02"
          P_TWO_SHORT = re.compile(r"\b0?\d{1,2}\s*[-_ ]\s*0?\d{1,2}\b")

          # Numeración al inicio: "10 - Titulo", "01. Titulo", "02) Titulo"
          P_LEADING_NUM = re.compile(r"^\s*\d{1,3}\s*[-–.)]\s+\S+", re.IGNORECASE)

          # Solo número (ej: "01") o número con símbolos
          P_ONLY_NUMBER = re.compile(r"^\s*\d{1,3}\s*$")
          P_ONLY_NUMBER_NOISY = re.compile(r"^\s*0?\d{1,3}\s*[-–.)]?\s*$")

          # Tags comunes de releases (más comunes en películas; resta un poco)
          P_RELEASE_TAGS = re.compile(r"\b(1080p|720p|2160p|4k|bluray|brrip|webrip|web-dl|hdr|dvdrip|cam)\b", re.IGNORECASE)

          def series_score(title: str, group_title: str) -> int:
              t = title or ""
              gt = group_title or ""
              tn = norm(t)
              gtn = norm(gt)

              score = 0

              # group-title hints
              if any(h in gtn for h in SERIES_GROUP_HINTS):
                  score += 4
              if any(h in gtn for h in MOVIE_GROUP_HINTS):
                  score -= 4

              # title-only number: casi seguro episodio suelto
              if P_ONLY_NUMBER.match(t) or P_ONLY_NUMBER_NOISY.match(t):
                  score += 5

              # temporada/season tokens
              if P_TEMPORADA.search(t) or P_TNN.search(t) or P_SNN.search(t):
                  score += 4

              # patrones de episodio (fuertes)
              if P_SXXEYY.search(t) or P_XXxYY.search(t):
                  score += 5
              if P_CAP.search(t) or P_EP.search(t):
                  score += 4

              # "E01" puede ser ambiguo, suma menos
              if P_E.search(t):
                  score += 2

              # "01 02" / "01-02" etc
              if P_TWO_SHORT.search(t):
                  score += 2

              # numeración al inicio: señal débil (solo suma poco)
              if P_LEADING_NUM.search(t):
                  score += 1

              # tags típicos de releases: resta un poco (para evitar falsos positivos)
              if P_RELEASE_TAGS.search(t):
                  score -= 1

              # si tiene un año, suele ser película (no siempre, pero ayuda)
              if extract_year(t):
                  score -= 1

              return score

          def looks_like_series(title: str, group_title: str) -> bool:
              return series_score(title, group_title) >= 4

          # ------------------ Validación URL online ------------------
          def alive_status(code: int) -> bool:
              return (200 <= code < 400) or code in (401, 403)

          def is_url_alive(url: str) -> bool:
              if not url:
                  return False
              u = url.strip()
              if not (u.startswith("http://") or u.startswith("https://")):
                  return False

              headers_head = {
                  "User-Agent": "Mozilla/5.0 (GitHubActions) m3u-check/1.0",
                  "Accept": "*/*",
              }
              headers_get = {
                  "User-Agent": "Mozilla/5.0 (GitHubActions) m3u-check/1.0",
                  "Accept": "*/*",
                  "Range": "bytes=0-0",
              }

              try:
                  r = requests.head(u, allow_redirects=True, timeout=(TIMEOUT_SEC, TIMEOUT_SEC), headers=headers_head)
                  if alive_status(r.status_code):
                      return True
                  # algunos servidores no soportan HEAD
                  if r.status_code in (400, 405, 501):
                      pass
                  else:
                      return False
              except requests.RequestException:
                  # probamos GET minimizado
                  pass

              try:
                  r = requests.get(u, allow_redirects=True, timeout=(TIMEOUT_SEC, TIMEOUT_SEC), headers=headers_get, stream=True)
                  ok = alive_status(r.status_code)
                  try:
                      r.close()
                  except Exception:
                      pass
                  return ok
              except requests.RequestException:
                  return False

          # ------------------ Descargar M3U ------------------
          resp = requests.get(M3U_URL, timeout=90)
          resp.raise_for_status()
          text = resp.text

          lines = [ln.strip() for ln in text.splitlines() if ln.strip()]

          movies_entries = []
          series_entries = []

          current_extinf = None
          current_attrs = None
          current_title = None

          for ln in lines:
              if ln.startswith("#EXTM3U"):
                  continue

              if ln.startswith("#EXTINF"):
                  current_extinf = ln
                  current_attrs, current_title = parse_extinf(ln)
                  continue

              # URL (línea siguiente al EXTINF)
              if current_extinf and not ln.startswith("#"):
                  video_url = ln.strip()
                  group_title = (current_attrs.get("group-title") or "").strip()
                  tvg_logo = (current_attrs.get("tvg-logo") or "").strip()

                  title = (current_title or "").strip()
                  if not title:
                      title = (current_attrs.get("tvg-name") or current_attrs.get("tvg-id") or "").strip()

                  # si no hay título o url, ignorar
                  if not video_url or not title:
                      current_extinf = None
                      current_attrs = None
                      current_title = None
                      continue

                  entry = {
                      "extinf": current_extinf,
                      "title": title,
                      "url": video_url,
                      "group_title": group_title,
                      "tvg_logo": tvg_logo,
                      "anio": extract_year(title),
                  }

                  if looks_like_series(title, group_title):
                      series_entries.append(entry)
                  else:
                      movies_entries.append(entry)

                  current_extinf = None
                  current_attrs = None
                  current_title = None

          # ------------------ Validar URLs y filtrar ------------------
          if CHECK_URLS:
              all_urls = {}
              for e in movies_entries + series_entries:
                  all_urls[e["url"]] = None

              print(f"Validando URLs únicas: {len(all_urls)} (threads={MAX_WORKERS}, timeout={TIMEOUT_SEC}s)")

              results = {}
              with ThreadPoolExecutor(max_workers=MAX_WORKERS) as ex:
                  fut_map = {ex.submit(is_url_alive, u): u for u in all_urls.keys()}
                  for fut in as_completed(fut_map):
                      u = fut_map[fut]
                      try:
                          results[u] = bool(fut.result())
                      except Exception:
                          results[u] = False

              def keep_alive(entries):
                  kept = []
                  for e in entries:
                      if results.get(e["url"], False):
                          kept.append(e)
                  return kept

              movies_entries = keep_alive(movies_entries)
              series_entries = keep_alive(series_entries)

              print(f"URLs vivas -> Películas: {len(movies_entries)} | Series: {len(series_entries)}")

          # ------------------ Escribir M3U separados ------------------
          def write_m3u(path, entries):
              out = ["#EXTM3U"]
              for e in entries:
                  out.append(e["extinf"])
                  out.append(e["url"])
              with open(path, "w", encoding="utf-8") as f:
                  f.write("\n".join(out) + "\n")

          write_m3u(os.path.join(OUTPUT_DIR, "peliculas.m3u"), movies_entries)
          write_m3u(os.path.join(OUTPUT_DIR, "series.m3u"), series_entries)

          # ------------------ JSON con TODOS los campos (como pediste) ------------------
          def sample_from_entry(e, tipo: str):
              return {
                  "name": e["title"],
                  "url": e["url"],
                  "icono": e["tvg_logo"] or "",     # tvg-logo -> icono
                  "iconoHorizontal": "",
                  "iconpng": "",
                  "type": tipo,                     # PELICULA o SERIE
                  "descripcion": "",
                  "anio": e.get("anio") or "",
                  "genero": "",
                  "duracion": ""
              }

          def to_grouped_records(entries, tipo: str):
              groups = OrderedDict()
              for e in entries:
                  g = e["group_title"]  # group-title -> record.name
                  if g not in groups:
                      groups[g] = []
                  groups[g].append(sample_from_entry(e, tipo))
              return [{"name": g, "samples": samples} for g, samples in groups.items()]

          movies_records = to_grouped_records(movies_entries, "PELICULA")
          series_records = to_grouped_records(series_entries, "SERIE")

          # ------------------ Split JSON de películas en partes ------------------
          def split_records_by_samples(records, max_samples):
              parts = []
              cur = []
              cur_count = 0

              for r in records:
                  s = r.get("samples", [])
                  if cur and (cur_count + len(s) > max_samples):
                      parts.append(cur)
                      cur = []
                      cur_count = 0
                  cur.append(r)
                  cur_count += len(s)

              if cur:
                  parts.append(cur)
              return parts

          parts = split_records_by_samples(movies_records, CHUNK_SIZE)

          if len(parts) == 1:
              with open(os.path.join(OUTPUT_DIR, "peliculas.json"), "w", encoding="utf-8") as f:
                  json.dump(parts[0], f, indent=2, ensure_ascii=False)
          else:
              for i, part in enumerate(parts, start=1):
                  with open(os.path.join(OUTPUT_DIR, f"peliculas_part{i:03d}.json"), "w", encoding="utf-8") as f:
                      json.dump(part, f, indent=2, ensure_ascii=False)

              manifest = {
                  "total_parts": len(parts),
                  "chunk_size_samples": CHUNK_SIZE,
                  "files": [f"peliculas_part{i:03d}.json" for i in range(1, len(parts) + 1)]
              }
              with open(os.path.join(OUTPUT_DIR, "peliculas_manifest.json"), "w", encoding="utf-8") as f:
                  json.dump(manifest, f, indent=2, ensure_ascii=False)

          # Series JSON completo
          with open(os.path.join(OUTPUT_DIR, "series.json"), "w", encoding="utf-8") as f:
              json.dump(series_records, f, indent=2, ensure_ascii=False)

          print(f"M3U URL: {M3U_URL}")
          print(f"Películas: {sum(len(r['samples']) for r in movies_records)} items (records={len(movies_records)})")
          print(f"Series: {sum(len(r['samples']) for r in series_records)} items (records={len(series_records)})")
          print(f"Películas JSON parts: {len(parts)} (chunk_size={CHUNK_SIZE})")
          print(f"Salida en: {OUTPUT_DIR}")
          PY

      - name: Commit and Push
        run: |
          git config user.name "github-actions[bot]"
          git config user.email "github-actions[bot]@users.noreply.github.com"
          git add "${{ github.event.inputs.output_dir }}"
          if ! git diff --cached --exit-code; then
            git commit -m "Convertir M3U a JSON y separar series/peliculas (afinada + url check)"
            git push
          else
            echo "No hay cambios"
          fi
